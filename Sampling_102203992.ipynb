{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gQTweKK2Iu9A",
        "outputId": "b3880dae-0aa0-4793-e86d-4910d3eb1ad6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Missing Values in Dataset:\n",
            "Time      0\n",
            "V1        0\n",
            "V2        0\n",
            "V3        0\n",
            "V4        0\n",
            "V5        0\n",
            "V6        0\n",
            "V7        0\n",
            "V8        0\n",
            "V9        0\n",
            "V10       0\n",
            "V11       0\n",
            "V12       0\n",
            "V13       0\n",
            "V14       0\n",
            "V15       0\n",
            "V16       0\n",
            "V17       0\n",
            "V18       0\n",
            "V19       0\n",
            "V20       0\n",
            "V21       0\n",
            "V22       0\n",
            "V23       0\n",
            "V24       0\n",
            "V25       0\n",
            "V26       0\n",
            "V27       0\n",
            "V28       0\n",
            "Amount    0\n",
            "Class     0\n",
            "dtype: int64\n",
            "\n",
            "Missing Values After Handling:\n",
            "Time      0\n",
            "V1        0\n",
            "V2        0\n",
            "V3        0\n",
            "V4        0\n",
            "V5        0\n",
            "V6        0\n",
            "V7        0\n",
            "V8        0\n",
            "V9        0\n",
            "V10       0\n",
            "V11       0\n",
            "V12       0\n",
            "V13       0\n",
            "V14       0\n",
            "V15       0\n",
            "V16       0\n",
            "V17       0\n",
            "V18       0\n",
            "V19       0\n",
            "V20       0\n",
            "V21       0\n",
            "V22       0\n",
            "V23       0\n",
            "V24       0\n",
            "V25       0\n",
            "V26       0\n",
            "V27       0\n",
            "V28       0\n",
            "Amount    0\n",
            "Class     0\n",
            "dtype: int64\n",
            "\n",
            "Dataset Columns: Index(['Time', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10',\n",
            "       'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20',\n",
            "       'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'Amount',\n",
            "       'Class'],\n",
            "      dtype='object')\n",
            "\n",
            "Feature Columns (X): Index(['Time', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10',\n",
            "       'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20',\n",
            "       'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'Amount'],\n",
            "      dtype='object')\n",
            "Target Column (y): Class\n",
            "\n",
            "Balanced Dataset Columns: Index(['Time', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10',\n",
            "       'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20',\n",
            "       'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'Amount',\n",
            "       'Target'],\n",
            "      dtype='object')\n",
            "\n",
            "Balanced Dataset Class Distribution:\n",
            "Target\n",
            "0    763\n",
            "1    763\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Sample Sizes for Different Margins of Error: [385, 601, 1068, 2401, 9604]\n",
            "Adjusted Sample Sizes: [308, 432, 629, 934, 1317]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Model Accuracies for Each Sampling Technique:\n",
            "Sampling Technique 1: {'M1': 0.9193548387096774, 'M2': 0.9032258064516129, 'M3': 0.9516129032258065, 'M4': 0.6451612903225806, 'M5': 0.8064516129032258}\n",
            "Sampling Technique 2: {'M1': 0.8725490196078431, 'M2': 0.9901960784313726, 'M3': 1.0, 'M4': 0.7352941176470589, 'M5': 0.8235294117647058}\n",
            "Sampling Technique 3: {'M1': 0.9365079365079365, 'M2': 0.9761904761904762, 'M3': 1.0, 'M4': 0.6587301587301587, 'M5': 0.873015873015873}\n",
            "Sampling Technique 4: {'M1': 0.9518716577540107, 'M2': 0.9786096256684492, 'M3': 1.0, 'M4': 0.6470588235294118, 'M5': 0.8181818181818182}\n",
            "Sampling Technique 5: {'M1': 0.9318181818181818, 'M2': 0.9848484848484849, 'M3': 1.0, 'M4': 0.6742424242424242, 'M5': 0.8712121212121212}\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import math\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Step 1: Download the dataset\n",
        "url = \"https://raw.githubusercontent.com/AnjulaMehto/Sampling_Assignment/main/Creditcard_data.csv\"\n",
        "data = pd.read_csv(url)\n",
        "\n",
        "# Check for missing values\n",
        "print(\"Missing Values in Dataset:\")\n",
        "print(data.isnull().sum())\n",
        "\n",
        "# Handle missing values\n",
        "# For numerical columns, fill with the mean\n",
        "numerical_columns = data.select_dtypes(include=[\"float64\", \"int64\"]).columns\n",
        "data[numerical_columns] = data[numerical_columns].fillna(data[numerical_columns].mean())\n",
        "\n",
        "# For categorical columns, fill with the mode\n",
        "categorical_columns = data.select_dtypes(include=[\"object\"]).columns\n",
        "if len(categorical_columns) > 0:\n",
        "    data[categorical_columns] = data[categorical_columns].apply(lambda x: x.fillna(x.mode()[0]))\n",
        "\n",
        "# Verify no missing values remain\n",
        "print(\"\\nMissing Values After Handling:\")\n",
        "print(data.isnull().sum())\n",
        "\n",
        "# Debugging: Display the column names of the dataset\n",
        "print(\"\\nDataset Columns:\", data.columns)\n",
        "\n",
        "# Step 2: Balance the dataset using SMOTE\n",
        "X = data.iloc[:, :-1]  # Features\n",
        "y = data.iloc[:, -1]   # Target\n",
        "\n",
        "# Debugging: Check the structure of X and y\n",
        "print(\"\\nFeature Columns (X):\", X.columns)\n",
        "print(\"Target Column (y):\", y.name)\n",
        "\n",
        "# Ensure target has no NaN values\n",
        "if y.isnull().sum() > 0:\n",
        "    y = y.fillna(y.mode()[0])\n",
        "\n",
        "smote = SMOTE(random_state=42)\n",
        "X_balanced, y_balanced = smote.fit_resample(X, y)\n",
        "\n",
        "# Ensure no NaN values in the balanced target\n",
        "y_balanced = pd.Series(y_balanced, name=\"Target\").fillna(y.mode()[0])\n",
        "\n",
        "# Create a balanced dataset\n",
        "balanced_data = pd.concat([pd.DataFrame(X_balanced, columns=X.columns), y_balanced], axis=1)\n",
        "\n",
        "# Debugging: Check the structure of the balanced dataset\n",
        "print(\"\\nBalanced Dataset Columns:\", balanced_data.columns)\n",
        "\n",
        "# Re-identify numerical and categorical columns in balanced data\n",
        "numerical_columns = balanced_data.select_dtypes(include=[\"float64\", \"int64\"]).columns\n",
        "categorical_columns = balanced_data.select_dtypes(include=[\"object\"]).columns\n",
        "\n",
        "# Check and handle NaN values in the balanced dataset\n",
        "balanced_data[numerical_columns] = balanced_data[numerical_columns].fillna(balanced_data[numerical_columns].mean())  # Fill numeric NaNs\n",
        "balanced_data[categorical_columns] = balanced_data[categorical_columns].apply(lambda x: x.fillna(x.mode()[0]))  # Fill with mode\n",
        "\n",
        "print(\"\\nBalanced Dataset Class Distribution:\")\n",
        "print(balanced_data['Target'].value_counts())\n",
        "\n",
        "# Step 3: Sample size detection formula\n",
        "def calculate_sample_size(z, p, e):\n",
        "    numerator = (z ** 2) * p * (1 - p)\n",
        "    denominator = e ** 2\n",
        "    return math.ceil(numerator / denominator)\n",
        "\n",
        "# Parameters\n",
        "z = 1.96  # 95% confidence level\n",
        "p = 0.5   # Proportion (assume 0.5 if unknown)\n",
        "e_values = [0.05, 0.04, 0.03, 0.02, 0.01]  # Margins of error\n",
        "\n",
        "# Calculate sample sizes for different margins of error\n",
        "sample_sizes = [calculate_sample_size(z, p, e) for e in e_values]\n",
        "print(\"\\nSample Sizes for Different Margins of Error:\", sample_sizes)\n",
        "\n",
        "# Adjust sample sizes for finite population (optional)\n",
        "def adjusted_sample_size(n, N):\n",
        "    return math.ceil(n / (1 + (n - 1) / N))\n",
        "\n",
        "N = len(balanced_data)\n",
        "adjusted_sample_sizes = [adjusted_sample_size(n, N) for n in sample_sizes]\n",
        "print(\"Adjusted Sample Sizes:\", adjusted_sample_sizes)\n",
        "\n",
        "# Step 4: Generate five samples using sampling techniques\n",
        "samples = []\n",
        "\n",
        "# Sampling techniques\n",
        "# 1. Random Sampling\n",
        "samples.append(balanced_data.sample(n=adjusted_sample_sizes[0], random_state=42))\n",
        "\n",
        "# 2. Systematic Sampling\n",
        "def systematic_sampling(data, step):\n",
        "    indexes = range(0, len(data), step)\n",
        "    return data.iloc[list(indexes)]\n",
        "\n",
        "samples.append(systematic_sampling(balanced_data, step=N // adjusted_sample_sizes[1]))\n",
        "\n",
        "# 3. Stratified Sampling\n",
        "_, stratified_sample = train_test_split(\n",
        "    balanced_data,\n",
        "    test_size=adjusted_sample_sizes[2] / N,\n",
        "    stratify=balanced_data['Target'],\n",
        "    random_state=42\n",
        ")\n",
        "samples.append(stratified_sample)\n",
        "\n",
        "\n",
        "# 4. Cluster Sampling\n",
        "clusters = balanced_data.groupby('Target')\n",
        "cluster_sample = pd.concat([\n",
        "    clusters.get_group(1).sample(n=adjusted_sample_sizes[3] // 2, random_state=42),\n",
        "    clusters.get_group(0).sample(n=adjusted_sample_sizes[3] // 2, random_state=42)\n",
        "], axis=0)\n",
        "samples.append(cluster_sample)\n",
        "\n",
        "\n",
        "# 5. Bootstrap Sampling\n",
        "samples.append(balanced_data.sample(n=adjusted_sample_sizes[4], replace=True, random_state=42))\n",
        "\n",
        "# Step 5: Train models on the samples\n",
        "def train_and_evaluate_model(model, X_train, X_test, y_train, y_test):\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    return accuracy_score(y_test, y_pred)\n",
        "\n",
        "results = []\n",
        "\n",
        "for i, sample in enumerate(samples):\n",
        "    # Split data\n",
        "    X_sample = sample.iloc[:, :-1]\n",
        "    y_sample = sample.iloc[:, -1]\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X_sample, y_sample, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Models\n",
        "    model_accuracies = {}\n",
        "\n",
        "    # Model M1: Logistic Regression\n",
        "    from sklearn.linear_model import LogisticRegression\n",
        "    lr = LogisticRegression(max_iter=1000, random_state=42)\n",
        "    model_accuracies['M1'] = train_and_evaluate_model(lr, X_train, X_test, y_train, y_test)\n",
        "\n",
        "    # Model M2: Decision Tree\n",
        "    from sklearn.tree import DecisionTreeClassifier\n",
        "    dt = DecisionTreeClassifier(random_state=42)\n",
        "    model_accuracies['M2'] = train_and_evaluate_model(dt, X_train, X_test, y_train, y_test)\n",
        "\n",
        "    # Model M3: Random Forest\n",
        "    rf = RandomForestClassifier(random_state=42)\n",
        "    model_accuracies['M3'] = train_and_evaluate_model(rf, X_train, X_test, y_train, y_test)\n",
        "\n",
        "    # Model M4: Support Vector Machine\n",
        "    from sklearn.svm import SVC\n",
        "    svm = SVC(random_state=42)\n",
        "    model_accuracies['M4'] = train_and_evaluate_model(svm, X_train, X_test, y_train, y_test)\n",
        "\n",
        "    # Model M5: K-Nearest Neighbors\n",
        "    from sklearn.neighbors import KNeighborsClassifier\n",
        "    knn = KNeighborsClassifier()\n",
        "    model_accuracies['M5'] = train_and_evaluate_model(knn, X_train, X_test, y_train, y_test)\n",
        "\n",
        "    results.append(model_accuracies)\n",
        "\n",
        "# Step 6: Display results\n",
        "print(\"\\nModel Accuracies for Each Sampling Technique:\")\n",
        "for i, result in enumerate(results):\n",
        "    print(f\"Sampling Technique {i+1}: {result}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rwxjc6ZwIvwD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}